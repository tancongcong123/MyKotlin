从业务场景中获取原始的数据，并把这些数据聚合起来，传输到服务器进行存储。
常见的数据采集方式：
    传感器 主要依赖各种硬件设备
    爬虫 
    日志
        日志采集的数据一般被分为两类：事件和属性
            事件 事件的收集一般是连贯的
                曝光事件
                点击事件
                用户停留事件
            属性 属性的收集一般是一次性的
                设备型号、版本、网络等
        日志采集的手段：数据埋点
            埋点的困难：来源多（Android、iOS、pc、小程序等）、页面多、数据格式不相同
            埋点的方式：手动、半自动、全自动
                全自动埋点从基本的事件和属性出发，把所有的东西都纳入埋点的范畴，但是这样收集全量信息，开销大、存储成本高
数据传输---消息系统kafka/Flume
    消息系统：把数据从一个地方发往另一个地方的工具
    kafka：高吞吐量、分布式发布/订阅消息系统、削峰填谷；基于zookeeper
        支持多种语言，如java、c、python、go、node.js、erlang等
        kafka集群在消息的生产者和消费者之间建立了联系机制，来保障消息的运输
        kafka要解决如何存储消息、如何进行集群调度实现负载均衡、如何保障通信等等问题
        基本结构：
            消息的生产者 producer 可以是App、服务或者各种sdk
            消息的消费者 consumer 每一个消费者都属于一个特定的小组(group)，同一个主题的一条消息只能被同一个消费组下的一个消费者消费，但是可以被不同消费组的消费者同时消费
                如需对消息进行多个消费者重复消费，就要配置多个消费组
            消息：kafka中传输的最基本的单元
            主题 topic
            分区和副本
            偏移量：可以理解为一种消息的索引
        侧重于数据的存储和流数据的实时处理，是一个追求高吞吐量、高负载的消息队列
    Flume：高可用、分布式的日志收集和传输系统
        概念：
            源 source：负责接收输入的数据，有两种工作模式（主动拉取数据；等待数据传输过来），在获取到数据之后把数据传递给channel
            通道 channel：是一个中间环节，是临时存储数据的部分，channel可以使用不同的配置比如内存、文件甚至数据库作为channel
            接收器 sink：是封装好的输出部分，选择不同类型的Sink，会将从channel获取的数据输出到不同的地方，比如HDFS输出时就选择HDFS Sink
            事件 event：传递一个数据单元即为一个事件
            代理 Agent：一个代理就是一个独立的运行单元，由Source、Channel、Sink组成，一个Agent中可能有多个组件
        侧重于数据的采集和传输，提供多种接口支持数据源的采集，但并不直接提供数据的持久化
    二者通常搭配使用，使用Flume进行采集，传输给kafka，再由kafka出输给计算框架MapReduce、Spark、Flink等，或者持久化到HDFS文件系统中